{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RL筆記\n",
    "===\n",
    "\n",
    "# 讓我們先回到機器學習的種類來談\n",
    "\n",
    "* 當前的機器學習種類\n",
    "1. 監督式學習（Supervised Learning）\n",
    "2. 無監督式學習（Unsupervised Learning）\n",
    "3. 強化學習（Reinforcement Learning）\n",
    "\n",
    "結構圖如下： \n",
    "\n",
    "![](http://img.blog.csdn.net/20170225171914631?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvY29mZmVlX2NyZWFt/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "## 對比監督式學習\n",
    "\n",
    "![](https://morvanzhou.github.io/static/results/ML-intro/RL3.png)\n",
    "\n",
    "監督式學習，是已經有明確的數據以及對應的正確標籤\n",
    "\n",
    "強化學習，沒有數據與標籤\n",
    "1. 透過一次又一次在環境中的嘗試，去取得這些數據以及標籤，再透過學習那些數據能夠對應那些標籤\n",
    "2. 透過學習到的規律，選擇高分的行為\n",
    "\n",
    "強化學習中，分數就是他的標籤，這點與監督式學習差不多。\n",
    "\n",
    "# RL簡介\n",
    "強化學習（Reinforcement learning）\n",
    "\n",
    "RL是一種通過試探而學習的範式，受人類學習新任務的啟發。\n",
    "在典型的RL設置中，AI被賦予在**數字環境中觀察**其當前狀態的任務，\n",
    "從環境接收每個動作的結果並給予激勵反饋，使得其知道動作是否促進或阻礙其進展。\n",
    "因此，AI必須找到最佳的獲得獎勵策略。谷歌旗下的DeepMind便使用了這種方法。\n",
    "在現實世界中，RL的一個例子是優化冷卻Google數據中心能效的任務，一個RL系統實現了減少40％的冷卻成本。在可以模擬的環境（例如視頻遊戲）中使用RL的優點是，**訓練數據可以以非常低的成本生成。這與監督深度學習任務形成鮮明對比，這些任務通常需要昂貴且難以從現實世界獲取的訓練數據。**\n",
    "\n",
    "    應用範圍：多個AI在自己的環境中學習互動，在相同的環境中互相學習導航的3D環境，如迷宮自動駕駛學習駕駛\n",
    "\n",
    "    公司：谷歌DeepMind，Prowler.io，Osaro，MicroPSI，Maluuba /微軟，NVIDIA，Mobileye。\n",
    "    \n",
    "# RL算法\n",
    "\n",
    "![](https://morvanzhou.github.io/static/results/ML-intro/RL4.png)\n",
    "\n",
    "強化學習是一個大家族，它包含了很多種算法\n",
    "\n",
    "之中一些比較有名的算法：\n",
    "\n",
    "1. 透過行為的價值選取特定行為的方法，使用表格學習的 q learning, sarsa, 使用神經網路學習的 deep q network\n",
    "\n",
    "2. 直接輸出行為的 policy gradients\n",
    "\n",
    "3. 了解所處的環境：想像出一個虛擬的環境並從虛擬的環境中學習\n",
    "\n",
    "RL:嘗試的過程中學習到特定環境選擇哪種行動可以得到最大回報\n",
    "當前行動會影響當前rewards，會影響之後的狀態和一系列的rewards\n",
    "\n",
    "\n",
    "RL最重要的3個特點：\n",
    "1. 基本是以一種閉環的形式\n",
    "2. 不會直接指示選擇哪種行動（actions）\n",
    "3. 一系列的actions和獎勵信號（reward signals）都會影響之後較長的時間。\n",
    "\n",
    "RL與監督式學習、無監督式學習的比較：\n",
    "1. 有監督的學習是從一個已經標記的訓練集中，訓練集中每一個樣本的特徵可以視為是對該情況的描述，而其label可以視為是應該執行的正確行動。 \n",
    "2. 但監督式學習不能學習交互的情景，因為在交互的問題中獲得期望行為的樣例是不實際的，agent只能從自己的經歷中進行學習，而經歷中採取的行為不一定是最優的\n",
    "3. 此時利用RL就非常合適，因為RL不是利用正確的行為來訓練，而是利用已有的訓練訊息來對行為進行評價\n",
    "4. RL利用的並不是採取正確行動的experience，從這一點來看和無監督式學習有點像，但不同之處在於：無監督式學習可以說是一堆未標記的樣本中發現隱藏結構，而RL的目的是最大化reward signal\n",
    "5. RL與其他機器學習算法最不同的地方在於：其中沒有監督者，只有一個reward信號，反饋是延遲的，不適立即生成的。時間在RL中具有重要意義，agent的行為會影像之後一系列的data\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "RL採用的是當前模型指導下一步的行動，下一步行動獲得reward之後再更新模型，不斷疊代重複直到模型收斂\n",
    "  \n",
    "RL模型中兩個非常重要的概念：\n",
    "探索（exploration）：選擇之前未執行過的actions，探索更多的可能性\n",
    "開發（exploitation）：選擇已執行過的actions，從而對已知的actions模型進行完善\n",
    "\n",
    "RL非常像是“trial-and-error learning”，在嘗試與試驗中發現好的policy。\n",
    "\n",
    ">就比如下图中的曲线代表函数f(x)，它是一个未知的[a,b]的连续函数，现在让你选择一个x使得f(x)取的最大值，规则是你可以通过自己给定x来查看其所对应的f(x)，假如通过在[a,0]之间的几次尝试你发现在接近x1的时候的值较大，于是你想通过在x1附近不断的尝试和逼近来寻找这个可能的“最大值”，这个就称为是exploitation，但是[0,b]之间就是个未探索过的未知的领域，这时选择若选择这一部分的点就称为是exploration，如果不进行exploration也许找到的只是个局部的极值。“exploration”与“exploitation”在RL中同样重要，如何在“exploration”与“exploitation”之间权衡是RL中的一个重要的问题和挑战‵\n",
    "\n",
    "![](http://img.blog.csdn.net/20170225172519948?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvY29mZmVlX2NyZWFt/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "\n",
    "\n",
    "## Markov Decision Processes 馬可夫性質\n",
    "(參考自 http://darren1231.pixnet.net/blog/post/336499256-ai%E6%95%99%E5%AD%B8----chap3%3Areinforcement-learning-problem )\n",
    "![](https://pic.pimg.tw/darren1231/1478507169-1373853504.png)\n",
    "系統的未來狀態s(t+1)只與現在狀態s(t)有關，而與之前狀態s(t-1),s(t-2).....無關，那什麼系統符合MDP呢？舉個簡單的例子，下棋就是一個MDP系統，你下一步該怎麼走s(t+1)只根據你現在的盤面狀態s(t)決定即可，跟你之前走的都沒關係，所以這也是AI常常會以下棋做研究的原因，因為他就是一個標準的MDP系統。了解大概什麼是MDP系統後，我們再細部講解馬可夫對這系統下了什麼定義，馬可夫總共定義了六種東西，分別是：\n",
    "\n",
    "* 一系列的狀態 (s)：\n",
    "狀態是可以千變萬化隨自己定義的，以上面的走迷宮圖片為例，我定義狀態為機器人到達的不同位置，因此這個環境就產生了12個不同的狀態。\n",
    "\n",
    "* 一系列的動作 (a)：\n",
    "同理也是可以自己定義的，在這個例子中我可以定義動作為上下左右\n",
    "\n",
    "* 狀態移轉方程式 T(s, a, s’)：\n",
    "紀錄著從狀態s採取動作a到達s’的機率，比如上圖機器人在(3,1)的位置上，機器人採取往上這個動作，正常情況下T(s(3,1), 上, s’(3,2))=1，但在現實生活中呢？你有沒有可能因為輪子打滑而造成採取同樣的動作卻抵達不同的狀態，這個T就是把這個因素考慮進來，所以你如果想要在模擬系統中把這個參數也考慮進去你就可以設T(s(3,1), 上, s’(3,2))=0.8，而分別各有0.1的機率打滑造成抵達狀態的左右兩邊，也就是說\n",
    "T(s(3,1), 上, s’(4,1))=0.1  不同表示法為：P(s’| s, a)=0.1\n",
    "T(s(3,1), 上, s’(2,1))=0.1\n",
    "\n",
    "* 獎勵函數方程式 R(s, a, s’) ：\n",
    "這個是RL學習成功與否的關鍵，你必須告訴機器人，從哪個狀態到哪個狀態是好的，設一個reward獎勵它，就像在教狗狗握手一樣，牠做對了就給牠東西吃(reward=positive)，牠做錯了就打牠一下(reward=negative)或擺個生氣的臉，這樣狗狗就知道做錯了，其實RL就是從這樣的一個學習模式演化而來，目的就是為了讓機器人學會某一件事(狗狗握手)，差別是我們把他數學化了，數學話的目的為了好進行公式推導演算，簡單的來說呢就是使機器人得到的reward最大化就是RL在解決的問題。拿這個地圖範例來說，我們想要機器人走到終點(4,3)，遠離火堆(4,2)，因此我們可以設定 \n",
    "R(s(3,3), 右, s’(4,3))=1\n",
    "R(s(3,2), 右, s’(4,2))=-1 ，來使機器人學會避開火堆到達終點的最短路徑。\n",
    "\n",
    "* 起始狀態(start state)：\n",
    "定義系統的初始狀態(1,1)\n",
    "\n",
    "* 結束狀態(terminal state)：\n",
    "定義系統的終點位置(4,3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "原文網址：https://kknews.cc/zh-tw/tech/96ema3j.html\n",
    "\n",
    "# Q-Learning\n",
    "\n",
    "# 參考資料\n",
    "* http://blog.csdn.net/coffee_cream/article/details/57085729\n",
    "* https://kknews.cc/zh-tw/tech/96ema3j.html\n",
    "* http://darren1231.pixnet.net/blog/post/336499256-ai%E6%95%99%E5%AD%B8----chap3%3Areinforcement-learning-problem\n",
    "* http://chenrudan.github.io/blog/2016/06/06/reinforcementlearninglesssion1.html\n",
    "* https://zh.wikipedia.org/wiki/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0\n",
    "* http://valser.org/thread-1431-1-1.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
